{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorboard_logger==0.1.0 in /usr/local/lib/python3.5/dist-packages\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.5/dist-packages (from tensorboard_logger==0.1.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.5/dist-packages (from tensorboard_logger==0.1.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.5/dist-packages (from tensorboard_logger==0.1.0)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.5/dist-packages (from tensorboard_logger==0.1.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.5/dist-packages (from tensorboard_logger==0.1.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.5/dist-packages (from protobuf->tensorboard_logger==0.1.0)\n",
      "Requirement already satisfied: olefile in /usr/local/lib/python3.5/dist-packages (from pillow>=4.1.1->tensorboard_logger==0.1.0)\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 10.0.0 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install tensorboard_logger==0.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import traceback\n",
    "\n",
    "import shutil\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from tensorboard_logger import Logger\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from settings.paths import FILTERED_MS_CELEB_IMAGES_DIR, \\\n",
    "                           SRGAN_MSE_LOSS_LOGS_DIR, \\\n",
    "                           SRGAN_VGG_LOSS_LOGS_DIR, \\\n",
    "                           SRGAN_LIGHT_CNN_LOSS_LOGS_DIR, \\\n",
    "                           SRGAN_MSE_LOSS_WEIGHTS_DIR, \\\n",
    "                           SRGAN_MSE_LOSS_BEST_WEIGHT, \\\n",
    "                           SRGAN_VGG_LOSS_GENERATOR_WEIGHTS_DIR, \\\n",
    "                           SRGAN_VGG_LOSS_DISCRIMINATOR_WEIGHTS_DIR, \\\n",
    "                           SRGAN_LIGHT_CNN_LOSS_GENERATOR_WEIGHTS_DIR, \\\n",
    "                           SRGAN_LIGHT_CNN_LOSS_DISCRIMINATOR_WEIGHTS_DIR\n",
    "                        \n",
    "                        \n",
    "from utils import maybe_mkdir, remove_if_exists, get_last_file\n",
    "\n",
    "from src.light_cnn import LightCNN_9Layers\n",
    "from src.srgan import Generator, Discriminator, FeatureExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '6, 7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GraysacleFeatureExtractor(FeatureExtractor):\n",
    "    def forward(self, x):\n",
    "        grayscale_x = 0.21 * x[:,0:1] + 0.72 * x[:,1:2] + 0.07 * x[:,2:3]\n",
    "        return super().forward(grayscale_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MSCeleb(ImageFolder):\n",
    "    def __init__(self, root):\n",
    "        super().__init__(root)\n",
    "        \n",
    "        self._to_tensor = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        self._downscale = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToPILImage(),\n",
    "                transforms.Resize(32, 0),\n",
    "                transforms.ToTensor(),\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image, _ = super().__getitem__(index)\n",
    "        \n",
    "        image_hr = self._to_tensor(image)\n",
    "        image_lr = self._downscale(image_hr)\n",
    "        \n",
    "        return image_hr, image_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = MSCeleb(FILTERED_MS_CELEB_IMAGES_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_loader = DataLoader(\n",
    "    dataset, batch_size=16, shuffle=True,\n",
    "    num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRGANManager:\n",
    "    def __init__(self,\n",
    "                 dataset_loader,\n",
    "                 n_resblocks=16, n_upsample=2,\n",
    "                 generator_lr=0.0001, discriminator_lr=0.0001,\n",
    "                 batch_size=16,\n",
    "                 cuda=True):\n",
    "        self._batch_size = batch_size\n",
    "        \n",
    "        self._generator = Generator(n_resblocks, n_upsample)\n",
    "        self._discriminator = Discriminator()\n",
    "        \n",
    "        self._content_criterion = nn.MSELoss()\n",
    "        self._adversarial_criterion = nn.BCELoss()\n",
    "        self._ones_const = Variable(torch.ones(BATCH_SIZE, 1))\n",
    "        \n",
    "        self._cuda = cuda\n",
    "        if self._cuda is True:\n",
    "            self._generator = nn.DataParallel(self._generator).cuda()\n",
    "            self._discriminator = nn.DataParallel(self._discriminator).cuda()\n",
    "            self._content_criterion = self._content_criterion.cuda()\n",
    "            self._adversarial_criterion = self._adversarial_criterion.cuda()\n",
    "            self._ones_const = self._ones_const.cuda()\n",
    "        \n",
    "        self._optim_generator = optim.Adam(\n",
    "            self._generator.parameters(), lr=generator_lr\n",
    "        )\n",
    "        self._optim_discriminator = optim.Adam(\n",
    "            self._discriminator.parameters(), lr=discriminator_lr\n",
    "        )\n",
    "        \n",
    "        self._dataset_loader = dataset_loader\n",
    "        \n",
    "    def load_generator_weights(self, path):\n",
    "        self._generator.load_state_dict(torch.load(path))\n",
    "        \n",
    "    def load_discriminator_weights(self, path):\n",
    "        self._discriminator.load_state_dict(torch.load(path))\n",
    "\n",
    "    def train_mse_only(self,\n",
    "                       log_dir=SRGAN_MSE_LOSS_LOGS_DIR,\n",
    "                       epoch_count=30,\n",
    "                       generator_weights_dir=SRGAN_MSE_LOSS_WEIGHTS_DIR,\n",
    "                       save_frequency=15000,\n",
    "                       values_log_frequency=300,\n",
    "                       images_log_frequency=3000,\n",
    "                       start_log=100,\n",
    "                       train_type='careful'):\n",
    "        self._train(\n",
    "            log_dir=log_dir,\n",
    "            epoch_count=epoch_count,\n",
    "            train_function=self._mse_only_train_function,\n",
    "            generator_weights_dir=generator_weights_dir,\n",
    "            save_frequency=save_frequency,\n",
    "            values_log_frequency=values_log_frequency,\n",
    "            images_log_frequency=images_log_frequency,\n",
    "            start_log=start_log,\n",
    "            train_type=train_type\n",
    "        )\n",
    "        \n",
    "    def _mse_only_train_function(self, high_res_real, high_res_fake):\n",
    "        self._generator.zero_grad()\n",
    "\n",
    "        generator_content_loss = self._content_criterion(\n",
    "            high_res_fake, high_res_real,\n",
    "        )\n",
    "\n",
    "        generator_content_loss.backward()\n",
    "        self._optim_generator.step()\n",
    "        \n",
    "        return {\n",
    "            'generator_mse_loss': generator_content_loss,\n",
    "        }\n",
    "    \n",
    "    def train_light_cnn_loss(self,\n",
    "                       log_dir=SRGAN_LIGHT_CNN_LOSS_LOGS_DIR,\n",
    "                       generator_weights_dir=SRGAN_LIGHT_CNN_LOSS_GENERATOR_WEIGHTS_DIR,\n",
    "                       discriminator_weights_dir=SRGAN_LIGHT_CNN_LOSS_DISCRIMINATOR_WEIGHTS_DIR,\n",
    "                       epoch_count=30,\n",
    "                       save_frequency=5000,\n",
    "                       values_log_frequency=100,\n",
    "                       images_log_frequency=1000,\n",
    "                       start_log=50,\n",
    "                       train_type='careful',\n",
    "                       load_pretrained_generator=SRGAN_MSE_LOSS_BEST_WEIGHT,\n",
    "                       features_weight=3600,\n",
    "                       image_weight=1,\n",
    "                       content_weight=1,\n",
    "                       adversarial_weight=1e-3):\n",
    "        \n",
    "        feature_extractor = GraysacleFeatureExtractor(\n",
    "            LightCNN_9Layers(), 6\n",
    "        )\n",
    "        \n",
    "        self._train_perceptual_loss(\n",
    "            log_dir=log_dir,\n",
    "            epoch_count=epoch_count,\n",
    "            generator_weights_dir=generator_weights_dir,\n",
    "            discriminator_weights_dir=discriminator_weights_dir,\n",
    "            feature_extractor=feature_extractor,\n",
    "            save_frequency=save_frequency,\n",
    "            values_log_frequency=values_log_frequency,\n",
    "            images_log_frequency=images_log_frequency,\n",
    "            start_log=start_log,\n",
    "            train_type=train_type,\n",
    "            load_pretrained_generator=load_pretrained_generator,\n",
    "            features_weight=features_weight,\n",
    "            image_weight=image_weight,\n",
    "            content_weight=content_weight,\n",
    "            adversarial_weight=adversarial_weight,\n",
    "        )\n",
    "    \n",
    "    def train_vgg_loss(self,\n",
    "                       log_dir=SRGAN_VGG_LOSS_LOGS_DIR,\n",
    "                       generator_weights_dir=SRGAN_VGG_LOSS_GENERATOR_WEIGHTS_DIR,\n",
    "                       discriminator_weights_dir=SRGAN_VGG_LOSS_DISCRIMINATOR_WEIGHTS_DIR,\n",
    "                       epoch_count=30,\n",
    "                       save_frequency=5000,\n",
    "                       values_log_frequency=100,\n",
    "                       images_log_frequency=1000,\n",
    "                       start_log=50,\n",
    "                       train_type='careful',\n",
    "                       load_pretrained_generator=SRGAN_MSE_LOSS_BEST_WEIGHT,\n",
    "                       features_weight=0.006,\n",
    "                       image_weight=1,\n",
    "                       content_weight=1,\n",
    "                       adversarial_weight=1e-3):\n",
    "        \n",
    "        feature_extractor = FeatureExtractor(\n",
    "            torchvision.models.vgg19(pretrained=True)\n",
    "        )\n",
    "        \n",
    "        self._train_perceptual_loss(\n",
    "            log_dir=log_dir,\n",
    "            epoch_count=epoch_count,\n",
    "            generator_weights_dir=generator_weights_dir,\n",
    "            discriminator_weights_dir=discriminator_weights_dir,\n",
    "            feature_extractor=feature_extractor,\n",
    "            save_frequency=save_frequency,\n",
    "            values_log_frequency=values_log_frequency,\n",
    "            images_log_frequency=images_log_frequency,\n",
    "            start_log=start_log,\n",
    "            train_type=train_type,\n",
    "            load_pretrained_generator=load_pretrained_generator,\n",
    "            features_weight=features_weight,\n",
    "            image_weight=image_weight,\n",
    "            content_weight=content_weight,\n",
    "            adversarial_weight=adversarial_weight,\n",
    "        )\n",
    "    \n",
    "    def _train_perceptual_loss(self,\n",
    "                               log_dir,\n",
    "                               generator_weights_dir,\n",
    "                               discriminator_weights_dir,\n",
    "                               epoch_count=30,\n",
    "                               feature_extractor=None,\n",
    "                               save_frequency=5000,\n",
    "                               values_log_frequency=100,\n",
    "                               images_log_frequency=1000,\n",
    "                               start_log=50,\n",
    "                               train_type='careful',\n",
    "                               load_pretrained_generator=SRGAN_MSE_LOSS_BEST_WEIGHT,\n",
    "                               features_weight=0.006,\n",
    "                               image_weight=1,\n",
    "                               content_weight=1,\n",
    "                               adversarial_weight=1e-3):\n",
    "        \n",
    "        assert feature_extractor is not None\n",
    "        \n",
    "        self._features_weight = features_weight\n",
    "        self._image_weight = image_weight\n",
    "        self._content_weight = content_weight\n",
    "        self._adversarial_weight = adversarial_weight\n",
    "        \n",
    "        self._feature_extractor = feature_extractor\n",
    "        \n",
    "        if self._cuda is True:\n",
    "            self._feature_extractor.cuda()\n",
    "        \n",
    "        if load_pretrained_generator is not None:\n",
    "            self.load_generator_weights(load_pretrained_generator)\n",
    "        \n",
    "        self._train(\n",
    "            log_dir=log_dir,\n",
    "            epoch_count=epoch_count,\n",
    "            train_function=self._perceptual_loss_train_function,\n",
    "            generator_weights_dir=generator_weights_dir,\n",
    "            discriminator_weights_dir=discriminator_weights_dir,\n",
    "            save_frequency=save_frequency,\n",
    "            values_log_frequency=values_log_frequency,\n",
    "            images_log_frequency=images_log_frequency,\n",
    "            start_log=start_log,\n",
    "            train_type=train_type\n",
    "        )\n",
    "        \n",
    "    def _perceptual_loss_train_function(self, high_res_real, high_res_fake):\n",
    "        # Additional variables\n",
    "        target_real = Variable(torch.rand(self._batch_size, 1) * 0.5 + 0.7)\n",
    "        target_fake = Variable(torch.rand(self._batch_size, 1) * 0.3)\n",
    "        \n",
    "        if self._cuda is True:\n",
    "            target_real = target_real.cuda()\n",
    "            target_fake = target_fake.cuda()\n",
    "        \n",
    "        # Train discriminator\n",
    "        self._discriminator.zero_grad()\n",
    "        \n",
    "        discriminator_loss = self._adversarial_criterion(\n",
    "            self._discriminator(high_res_real), target_real\n",
    "        ) + self._adversarial_criterion(\n",
    "            self._discriminator(high_res_fake.detach()), target_fake\n",
    "        )\n",
    "        \n",
    "        discriminator_loss.backward()\n",
    "        self._optim_discriminator.step()\n",
    "        \n",
    "        # Train generator \n",
    "        self._generator.zero_grad()\n",
    "        \n",
    "        real_features = self._feature_extractor(high_res_real).detach()\n",
    "        fake_features = self._feature_extractor(high_res_fake)\n",
    "        \n",
    "        generator_content_loss_image = self._content_criterion(\n",
    "            high_res_fake, high_res_real\n",
    "        )\n",
    "        generator_content_loss_features = self._content_criterion(\n",
    "            fake_features, real_features\n",
    "        )\n",
    "        \n",
    "        generator_content_loss_total = \\\n",
    "            self._image_weight * generator_content_loss_image + \\\n",
    "            self._features_weight * generator_content_loss_features\n",
    "        \n",
    "        generator_adversarial_loss = self._adversarial_criterion(\n",
    "            self._discriminator(high_res_fake), self._ones_const\n",
    "        )\n",
    "\n",
    "        generator_total_loss = \\\n",
    "            self._content_weight * generator_content_loss_total + \\\n",
    "            self._adversarial_weight * generator_adversarial_loss\n",
    "\n",
    "        generator_total_loss.backward()\n",
    "        self._optim_generator.step() \n",
    "        \n",
    "        return {\n",
    "            'discriminator_loss': discriminator_loss,\n",
    "            'generator_adversarial_loss': generator_adversarial_loss,\n",
    "            'generator_content_loss_image': generator_content_loss_image,\n",
    "            'generator_content_loss_features': generator_content_loss_features,\n",
    "            'generator_content_loss_total': generator_content_loss_total,\n",
    "            'generator_total_loss': generator_total_loss,\n",
    "        }\n",
    "        \n",
    "    def _train(self,\n",
    "               log_dir,\n",
    "               epoch_count,\n",
    "               train_function,\n",
    "               generator_weights_dir,\n",
    "               save_frequency=15000,\n",
    "               values_log_frequency=300,\n",
    "               images_log_frequency=3000,\n",
    "               start_log=100,\n",
    "               train_type='careful',\n",
    "               discriminator_weights_dir=None):\n",
    "        \n",
    "        step_num = self._prepare_train_type(\n",
    "            train_type, log_dir,\n",
    "            generator_weights_dir, discriminator_weights_dir\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            logger = Logger(log_dir)\n",
    "\n",
    "            for epoch_number in range(epoch_count):\n",
    "                for image_hr, image_lr in tqdm(self._dataset_loader,\n",
    "                                               'Epoch {}'.format(epoch_number + 1),\n",
    "                                               total=len(self._dataset_loader)):\n",
    "                    high_res_real = Variable(image_hr)\n",
    "                    low_res = Variable(image_lr)\n",
    "                    high_res_fake = self._generator(low_res)\n",
    "                    if self._cuda is True:\n",
    "                        high_res_real = high_res_real.cuda()\n",
    "                        high_res_fake = high_res_fake.cuda()\n",
    "\n",
    "                    values_dict = train_function(high_res_real, high_res_fake)\n",
    "\n",
    "                    if step_num >= start_log:\n",
    "                        if step_num % values_log_frequency == 0:\n",
    "                            for key, value in values_dict.items():\n",
    "                                logger.log_value(\n",
    "                                    key,\n",
    "                                    value,\n",
    "                                    step_num\n",
    "                                )\n",
    "\n",
    "                        if step_num % images_log_frequency == 0:\n",
    "                            logger.log_images(\n",
    "                                'real_images',\n",
    "                                high_res_real.data.cpu()[:1],\n",
    "                                step_num\n",
    "                            )\n",
    "\n",
    "                            logger.log_images(\n",
    "                                'lr_images',\n",
    "                                low_res.data.cpu()[:1],\n",
    "                                step_num\n",
    "                            )\n",
    "\n",
    "                            logger.log_images(\n",
    "                                'fake_images',\n",
    "                                high_res_fake.data.cpu()[:1],\n",
    "                                step_num\n",
    "                            )\n",
    "\n",
    "                    if step_num % save_frequency == 0 and step_num != 0:\n",
    "                        self._save_all(step_num, generator_weights_dir, discriminator_weights_dir)\n",
    "                    step_num += 1\n",
    "                self._save_all(step_num, generator_weights_dir, discriminator_weights_dir)\n",
    "        except BaseException:\n",
    "            print(traceback.format_exc())\n",
    "            self._save_all(step_num, generator_weights_dir, discriminator_weights_dir)\n",
    "        \n",
    "    def _save_all(self, step_num, generator_weights_dir, discriminator_weights_dir=None):\n",
    "        torch.save(\n",
    "            self._generator.state_dict(), self._make_weight_path(generator_weights_dir, step_num)\n",
    "        )\n",
    "        \n",
    "        if discriminator_weights_dir is not None:\n",
    "            torch.save(\n",
    "                self._discriminator.state_dict(), self._make_weight_path(discriminator_weights_dir, step_num)\n",
    "            )\n",
    "        \n",
    "    def _prepare_train_type(self, train_type, log_dir,\n",
    "                            generator_weights_dir, discriminator_weights_dir=None):\n",
    "        step = 0\n",
    "        \n",
    "        if train_type is 'careful':\n",
    "            if os.path.exists(log_dir) or os.path.exists(generator_weights_dir) or \\\n",
    "               (discriminator_weights_dir is not None and os.path.exists(discriminator_weights_dir)):\n",
    "                raise AssertionError(\n",
    "                    'There are previous train files:\\nlog_dir: {}\\ngenerator_weights_dir: {}\\ndiscriminator_weights_dir: {}'.format(\n",
    "                        log_dir,\n",
    "                        generator_weights_dir,\n",
    "                        discriminator_weights_dir,\n",
    "                    )\n",
    "                )\n",
    "        elif train_type is 'clean':\n",
    "            print(\n",
    "                'Clean all train data for (Y/N):\\nlog_dir: {}\\ngenerator_weights_dir: {}\\ndiscriminator_weights_dir: {}'.format(\n",
    "                    log_dir,\n",
    "                    generator_weights_dir,\n",
    "                    discriminator_weights_dir,\n",
    "                )\n",
    "            )\n",
    "            answer = input()\n",
    "            if answer == 'Y':\n",
    "                remove_if_exists(log_dir)\n",
    "                remove_if_exists(generator_weights_dir)\n",
    "                if discriminator_weights_dir is not None:\n",
    "                    remove_if_exists(discriminator_weights_dir)\n",
    "            else:\n",
    "                raise Exception('Bad answer')\n",
    "        elif train_type is 'continue':\n",
    "            try:\n",
    "                filename = get_last_file(generator_weights_dir)\n",
    "                self.load_generator_weights(os.path.join(generator_weights_dir, filename))\n",
    "\n",
    "                if discriminator_weights_dir is not None:\n",
    "                    filename = get_last_file(discriminator_weights_dir)\n",
    "                    self.load_discriminator_weights(os.path.join(discriminator_weights_dir, filename))\n",
    "                \n",
    "                step = int(os.path.splitext(filename)[0])\n",
    "            except Exception:\n",
    "                raise Exception('Nothing to load')\n",
    "        else:\n",
    "            raise ValueError('No such train type')\n",
    "            \n",
    "        pathlib.Path(generator_weights_dir).mkdir(parents=True, exist_ok=True)\n",
    "        if discriminator_weights_dir is not None:\n",
    "            pathlib.Path(discriminator_weights_dir).mkdir(parents=True, exist_ok=True)\n",
    "        pathlib.Path(log_dir).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        return step\n",
    "        \n",
    "    def _make_weight_path(self, folder, step):\n",
    "        return os.path.join(folder, '{:010d}.pth'.format(step))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "srgan_manager = SRGANManager(dataset_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\r",
      "  0%|          | 0/287187 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 1/287187 [00:00<49:35:23,  1.61it/s]\u001b[A\n",
      "  0%|          | 2/287187 [00:00<37:06:53,  2.15it/s]\u001b[A\n",
      "  0%|          | 4/287187 [00:00<28:11:47,  2.83it/s]\u001b[A\n",
      "  0%|          | 6/287187 [00:01<21:36:46,  3.69it/s]\u001b[A\n",
      "  0%|          | 8/287187 [00:01<16:57:47,  4.70it/s]\u001b[A\n",
      "  0%|          | 10/287187 [00:01<14:07:16,  5.65it/s]\u001b[A\n",
      "  0%|          | 12/287187 [00:01<11:51:25,  6.73it/s]\u001b[A\n",
      "  0%|          | 14/287187 [00:01<10:12:19,  7.82it/s]\u001b[A\n",
      "  0%|          | 16/287187 [00:01<9:15:05,  8.62it/s] \u001b[A\n",
      "  0%|          | 18/287187 [00:02<8:36:18,  9.27it/s]\u001b[A\n",
      " 21%|██        | 59307/287187 [1:34:13<6:04:02, 10.43it/s] Process Process-13:\n",
      "Process Process-15:\n",
      "Process Process-16:\n",
      "Process Process-14:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 50, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Exception ignored in: <bound method DataLoaderIter.__del__ of <torch.utils.data.dataloader.DataLoaderIter object at 0x7ff224748d68>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 333, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 319, in _shutdown_workers\n",
      "    self.data_queue.get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 345, in get\n",
      "    return ForkingPickler.loads(res)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/multiprocessing/reductions.py\", line 70, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/resource_sharer.py\", line 57, in detach\n",
      "    with _resource_sharer.get_connection(self._id) as conn:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/resource_sharer.py\", line 87, in get_connection\n",
      "    c = Client(address, authkey=process.current_process().authkey)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 487, in Client\n",
      "    c = SocketClient(address)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 614, in SocketClient\n",
      "    s.connect(address)\n",
      "FileNotFoundError: [Errno 2] No such file or directory\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 50, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 50, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 50, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 343, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "srgan_manager.train_mse_only(epoch_count=30, train_type='continue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1:   0%|          | 0/287187 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1:   0%|          | 1/287187 [00:00<75:01:04,  1.06it/s]\u001b[A\n",
      "Epoch 1:   0%|          | 2/287187 [00:01<58:12:06,  1.37it/s]\u001b[A\n",
      "Epoch 1:   0%|          | 3/287187 [00:01<45:54:58,  1.74it/s]\u001b[A\n",
      "Epoch 1:   0%|          | 4/287187 [00:01<37:26:03,  2.13it/s]\u001b[A\n",
      "Epoch 1:   0%|          | 5/287187 [00:01<31:28:21,  2.53it/s]\u001b[A\n",
      "Epoch 1:   0%|          | 6/287187 [00:02<27:31:07,  2.90it/s]\u001b[A\n",
      "Epoch 1:   0%|          | 7/287187 [00:02<24:32:41,  3.25it/s]\u001b[A\n",
      "Epoch 1:   0%|          | 8/287187 [00:02<22:33:41,  3.54it/s]\u001b[A\n",
      "Epoch 1:   0%|          | 9/287187 [00:02<21:02:06,  3.79it/s]\u001b[A\n",
      "Epoch 1:   0%|          | 10/287187 [00:02<19:52:18,  4.01it/s]\u001b[A\n",
      "Epoch 1:   0%|          | 11/287187 [00:03<19:06:10,  4.18it/s]\u001b[A\n",
      "Epoch 1:   0%|          | 12/287187 [00:03<18:20:25,  4.35it/s]\u001b[A\n",
      "Epoch 1:   0%|          | 13/287187 [00:03<17:51:38,  4.47it/s]\u001b[A\n",
      "Epoch 1:   0%|          | 14/287187 [00:03<17:38:54,  4.52it/s]\u001b[A\n",
      "Epoch 1:   0%|          | 15/287187 [00:04<17:26:31,  4.57it/s]\u001b[A\n",
      "Epoch 1:   0%|          | 16/287187 [00:04<17:20:53,  4.60it/s]\u001b[A\n",
      "Epoch 1:   0%|          | 17/287187 [00:04<17:24:27,  4.58it/s]\u001b[A\n",
      "Epoch 1:   0%|          | 18/287187 [00:04<17:19:51,  4.60it/s]\u001b[A\n",
      "Epoch 1:   0%|          | 19/287187 [00:04<17:15:14,  4.62it/s]\u001b[A\n",
      "Epoch 1:   0%|          | 20/287187 [00:05<17:15:04,  4.62it/s]\u001b[A\n",
      "Epoch 1:   0%|          | 21/287187 [00:05<17:15:02,  4.62it/s]\u001b[A\n",
      "Epoch 1:   0%|          | 22/287187 [00:05<17:11:50,  4.64it/s]\u001b[A\n",
      "Epoch 1:   0%|          | 23/287187 [00:05<17:11:09,  4.64it/s]\u001b[A\n",
      "Epoch 1:   0%|          | 24/287187 [00:06<24:09:33,  3.30it/s]\u001b[A\n",
      "Epoch 1:   0%|          | 25/287187 [00:06<22:03:59,  3.61it/s]\u001b[A\n",
      "Epoch 1:   0%|          | 26/287187 [00:06<20:52:56,  3.82it/s]\u001b[A\n",
      "Epoch 1:   0%|          | 27/287187 [00:06<19:47:03,  4.03it/s]\u001b[A\n",
      "Epoch 1:   0%|          | 28/287187 [00:07<18:53:20,  4.22it/s]\u001b[A\n",
      "Epoch 1:   0%|          | 29/287187 [00:07<18:16:59,  4.36it/s]\u001b[A\n",
      "Epoch 1:   0%|          | 30/287187 [00:07<17:58:27,  4.44it/s]\u001b[A\n",
      "Epoch 1:   0%|          | 31/287187 [00:07<17:46:29,  4.49it/s]\u001b[A\n",
      "Epoch 1:   0%|          | 32/287187 [00:07<17:45:48,  4.49it/s]\u001b[A\n",
      "Epoch 1:   0%|          | 33/287187 [00:08<17:55:50,  4.45it/s]\u001b[A\n",
      "Epoch 1:   0%|          | 34/287187 [00:08<18:01:07,  4.43it/s]\u001b[A\n",
      "Epoch 1:   0%|          | 35/287187 [00:08<17:36:46,  4.53it/s]\u001b[A\n",
      "Epoch 1:   0%|          | 36/287187 [00:08<17:42:05,  4.51it/s]\u001b[A\n",
      "Epoch 1:   0%|          | 37/287187 [00:09<17:29:15,  4.56it/s]\u001b[A\n",
      "Epoch 1:   0%|          | 38/287187 [00:09<17:29:52,  4.56it/s]\u001b[A\n",
      "Epoch 1:   0%|          | 272/287187 [01:00<17:18:19,  4.61it/s]Process Process-17:\n",
      "Process Process-19:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Process Process-20:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Process Process-18:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-35-0160fb21dd30>\", line 278, in _train\n",
      "    high_res_fake = self._generator(low_res)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\", line 357, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/nn/parallel/data_parallel.py\", line 73, in forward\n",
      "    outputs = self.parallel_apply(replicas, inputs, kwargs)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/nn/parallel/data_parallel.py\", line 83, in parallel_apply\n",
      "    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/nn/parallel/parallel_apply.py\", line 59, in parallel_apply\n",
      "    thread.join()\n",
      "  File \"/usr/lib/python3.5/threading.py\", line 1054, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/usr/lib/python3.5/threading.py\", line 1070, in _wait_for_tstate_lock\n",
      "    elif lock.acquire(block, timeout):\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Exception ignored in: <bound method DataLoaderIter.__del__ of <torch.utils.data.dataloader.DataLoaderIter object at 0x7ff905293f60>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 333, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 319, in _shutdown_workers\n",
      "    self.data_queue.get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 345, in get\n",
      "    return ForkingPickler.loads(res)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/multiprocessing/reductions.py\", line 70, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/resource_sharer.py\", line 57, in detach\n",
      "    with _resource_sharer.get_connection(self._id) as conn:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/resource_sharer.py\", line 87, in get_connection\n",
      "    c = Client(address, authkey=process.current_process().authkey)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 487, in Client\n",
      "    c = SocketClient(address)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 614, in SocketClient\n",
      "    s.connect(address)\n",
      "FileNotFoundError: [Errno 2] No such file or directory\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 50, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 55, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 55, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"<ipython-input-5-ee4a06afb3b6>\", line 23, in __getitem__\n",
      "    image_lr = self._downscale(image_hr)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torchvision/transforms/transforms.py\", line 42, in __call__\n",
      "    img = t(img)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 50, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torchvision/transforms/transforms.py\", line 92, in __call__\n",
      "    return F.to_pil_image(pic, self.mode)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 50, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 343, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torchvision/transforms/functional.py\", line 100, in to_pil_image\n",
      "    pic = pic.mul(255).byte()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/tensor.py\", line 77, in byte\n",
      "    return self.type(type(self).__module__ + '.ByteTensor')\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/_utils.py\", line 38, in _type\n",
      "    return new_type(self.size()).copy_(self, async)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "srgan_manager.train_vgg_loss(epoch_count=30, train_type='continue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean all train data for (Y/N):\n",
      "log_dir: /home/data/alpus/clean_fsr/data/logs/srgan/light_cnn_loss\n",
      "generator_weights_dir: /home/data/alpus/clean_fsr/data/weights/srgan/light_cnn_loss/generator\n",
      "discriminator_weights_dir: /home/data/alpus/clean_fsr/data/weights/srgan/light_cnn_loss/discriminator\n",
      "Y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1:   0%|          | 0/287187 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1:   0%|          | 1/287187 [00:00<65:41:46,  1.21it/s]\u001b[A\n",
      "Epoch 1:   0%|          | 2/287187 [00:01<51:18:09,  1.55it/s]\u001b[A\n",
      "Epoch 1:   0%|          | 3/287187 [00:01<41:16:13,  1.93it/s]\u001b[A\n",
      "Epoch 1:   0%|          | 4/287187 [00:01<34:26:55,  2.32it/s]\u001b[A\n",
      "Epoch 1:   0%|          | 5/287187 [00:01<29:30:59,  2.70it/s]\u001b[A\n",
      "Epoch 1:   0%|          | 6/287187 [00:01<25:58:27,  3.07it/s]\u001b[A\n",
      "Epoch 1:   0%|          | 7/287187 [00:02<23:29:17,  3.40it/s]\u001b[A\n",
      "Epoch 1:   0%|          | 8/287187 [00:02<21:47:34,  3.66it/s]\u001b[A\n",
      "Epoch 1:   0%|          | 9/287187 [00:02<20:47:41,  3.84it/s]\u001b[A\n",
      "Epoch 1:   0%|          | 10/287187 [00:02<20:05:52,  3.97it/s]\u001b[A\n",
      "Epoch 1:   0%|          | 11/287187 [00:03<19:48:56,  4.03it/s]\u001b[A\n",
      "Epoch 1:   0%|          | 12/287187 [00:03<19:07:48,  4.17it/s]\u001b[A\n",
      "Epoch 1:   0%|          | 13/287187 [00:03<18:45:32,  4.25it/s]\u001b[A\n",
      "Epoch 1:   0%|          | 14/287187 [00:03<18:36:20,  4.29it/s]\u001b[A\n",
      "Epoch 1:   0%|          | 15/287187 [00:03<18:22:38,  4.34it/s]\u001b[A\n",
      "Epoch 1:   3%|▎         | 8076/287187 [32:23<27:04:55,  2.86it/s]"
     ]
    }
   ],
   "source": [
    "srgan_manager.train_light_cnn_loss(epoch_count=30, train_type='continue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
